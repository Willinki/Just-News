{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:37:02.647661Z",
     "start_time": "2020-09-04T09:37:01.053113Z"
    }
   },
   "outputs": [],
   "source": [
    "# General purpose\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import itertools\n",
    "from numpy.linalg import norm\n",
    "from collections import Counter\n",
    "import codecs\n",
    "import tqdm\n",
    "# NLP\n",
    "from cade.cade import CADE\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "# Machine Learning\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# currently installed theme will be used to\n",
    "# set plot style if no arguments provided\n",
    "# Theme\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:37:02.651826Z",
     "start_time": "2020-09-04T09:37:02.649426Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to remove punctuation from strings.\n",
    "# Copied from Prof.\n",
    "def simple_preproc(text):\n",
    "  \"\"\"\n",
    "  see: https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string\n",
    "  \"\"\"\n",
    "  return text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Pre-processing\n",
    "\n",
    "Here we load the json arrays from Mongo and output a csv file, i.e. a pandas dataframe. \n",
    "This DF will be used for the CADE training.\n",
    "\n",
    "Provo senza Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T08:05:45.159363Z",
     "start_time": "2020-09-04T08:05:42.811559Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data_final/scraping_data_Breitbart.json\n",
      "./data_final/scraping_data_NYTimes.json\n"
     ]
    }
   ],
   "source": [
    "df_list = pd.DataFrame()\n",
    "for filename in glob.glob('../data_final/*.json'):\n",
    "    print(filename)\n",
    "    with open(filename, 'r') as f:\n",
    "        json_load = json.loads(f.read())\n",
    "        df_list = df_list.append(pd.DataFrame.from_records(json_load, index=\"_id\"), ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:37:08.930297Z",
     "start_time": "2020-09-01T13:37:08.895411Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_list = df_list[['Title', \"Date\", \"Link\", \"Paragraphs\", \"Authors\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:37:09.750994Z",
     "start_time": "2020-09-01T13:37:09.747077Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_newssite(link = None):\n",
    "        if link is np.nan:\n",
    "            return(\"Wikipedia\")\n",
    "        elif \"slate.com\" in link:\n",
    "            return(\"Slate\")\n",
    "        elif \"https://www.nytimes.com\" in link:\n",
    "            return(\"New York Times\")\n",
    "        elif \"https://www.breitbart.com\" in link:\n",
    "            return(\"Breitbart\")\n",
    "        elif \"https://www.cnn.com\" in link:\n",
    "            return(\"CNN\")\n",
    "        elif \"abcnews\" in link:\n",
    "            return(\"ABC News\")\n",
    "        elif \"https://thefederalist.com\" in link:\n",
    "            return(\"The Federalist\")\n",
    "        elif \"https://www.newsmax.com\" in link:\n",
    "            return(\"News Max\")\n",
    "        else:\n",
    "            return(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:37:13.426602Z",
     "start_time": "2020-09-01T13:37:13.377277Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVVOCATO CI SIAMO\n"
     ]
    }
   ],
   "source": [
    "df_list[\"Newssite\"] = df_list[\"Link\"].apply(create_newssite)\n",
    "\n",
    "if len(df_list[df_list[\"Newssite\"][:] == \"Unknown\"]) == 0:\n",
    "    print(\"AVVOCATO CI SIAMO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T11:00:26.464683Z",
     "start_time": "2020-09-01T11:00:20.623411Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(\"./data_safe.csv\", 'w') as file:\n",
    "    df_list.to_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CADE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Embedding totale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First, we create a big text with everything. Like, EVERYTHING."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T09:14:58.811539Z",
     "start_time": "2020-09-03T09:14:51.736010Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(\"./data_safe.csv\") as file:\n",
    "    df = pd.read_csv(file, engine='c')\n",
    "\n",
    "df[\"Paragraphs\"] = df[\"Paragraphs\"].apply(literal_eval)\n",
    "\n",
    "EVERYTHING = \"\"\n",
    "for k in df[\"Paragraphs\"]:\n",
    "    EVERYTHING += \"\\n\".join(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T09:15:15.696792Z",
     "start_time": "2020-09-03T09:14:58.813419Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "EVERYTHING = simple_preproc(EVERYTHING).lower()\n",
    "EVERYTHING = EVERYTHING.replace(\"—\", \n",
    "                                                \"\").replace(\"\\n\", \n",
    "                                                            \" \").replace(\"“\", \n",
    "                                                                         \"\").replace(\"“\", \n",
    "                                                                                          \"\").replace(\"”\", \n",
    "                                                                                                      \"\").replace(\"’s\", \n",
    "                                                                                                             \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T09:15:16.150852Z",
     "start_time": "2020-09-03T09:15:15.699831Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(\"./EVERYTHING.txt\", 'w') as file:\n",
    "    file.write(EVERYTHING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:40:41.662023Z",
     "start_time": "2020-09-04T09:37:43.301198Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the compass from scratch.\n"
     ]
    }
   ],
   "source": [
    "aligner = CADE(size=100, workers=12)\n",
    "\n",
    "aligner.train_compass(\"./EVERYTHING.txt\", overwrite=False) # keep an eye on the overwrite behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Embedding singolo slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:41:03.786670Z",
     "start_time": "2020-09-04T09:40:41.664055Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Progress:1/2\n",
      "[INFO] Progress:2/2\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data_safe.csv\") as file:\n",
    "    df = pd.read_csv(file, engine='c')\n",
    "    \n",
    "df[\"Paragraphs\"] = df[\"Paragraphs\"].apply(literal_eval)\n",
    "\n",
    "for i, Newssite in enumerate(df[\"Newssite\"].unique()):\n",
    "    print(\"[INFO] Progress:\"+str(i+1)+\"/\"+str(len(df[\"Newssite\"].unique())))\n",
    "    newssite_to_text = \"\"\n",
    "    for k in df[df[\"Newssite\"] == Newssite][\"Paragraphs\"][:]:\n",
    "        newssite_to_text += \"\\n\".join(k)\n",
    "    newssite_to_text = simple_preproc(newssite_to_text).lower()\n",
    "    newssite_to_text = newssite_to_text.replace(\"—\", \n",
    "                                                \"\").replace(\"\\n\", \n",
    "                                                            \" \").replace(\"“\", \n",
    "                                                                         \"\").replace(\"“\", \n",
    "                                                                                          \"\").replace(\"”\", \n",
    "                                                                                                      \"\").replace(\"’s\", \n",
    "                                                                                                             \"\")\n",
    "    with open(\"./text_\"+str(Newssite)+\".txt\", 'w') as file:\n",
    "        file.write(newssite_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:44:36.883108Z",
     "start_time": "2020-09-04T09:41:03.788841Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training embeddings: slice ./text_Breitbart.txt.\n",
      "Initializing embeddings from compass.\n",
      "Training embeddings: slice ./text_New York Times.txt.\n",
      "Initializing embeddings from compass.\n"
     ]
    }
   ],
   "source": [
    "slices = {\n",
    "            Newssite: aligner.train_slice(\"./text_\"+str(Newssite)+\".txt\", save=False)\n",
    "            for Newssite in df[\"Newssite\"].unique()\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:44:39.279042Z",
     "start_time": "2020-09-04T09:44:36.885057Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Dobbiamo andare a salvare i vari slice\n",
    "for my_slice in slices:\n",
    "    slices[my_slice].save(\"models/\"+str(my_slice)+\".model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Visualizzazione medie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T09:21:37.104713Z",
     "start_time": "2020-09-03T09:21:37.101565Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sum_vectors(my_slice = None):\n",
    "    my_sum = 0\n",
    "    for i, word in enumerate(my_slice.wv.vocab):\n",
    "        my_sum += my_slice.wv[word]\n",
    "    return(my_sum/len(my_slice.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T09:21:37.681424Z",
     "start_time": "2020-09-03T09:21:37.107515Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_sum = [sum_vectors(slices[my_slice]) \n",
    "       for i, my_slice in enumerate(slices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T09:21:37.864532Z",
     "start_time": "2020-09-03T09:21:37.857862Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.   0.93]\n",
      " [0.93 1.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array([[int((1 - cosine(my_sum[j], my_sum[i]))*100)/100\n",
    "  for j in range(len(slices))] \n",
    " for i in range(len(slices))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T09:21:37.876932Z",
     "start_time": "2020-09-03T09:21:37.866199Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1.0\n",
      "0 1\n",
      "0.9388323426246643\n",
      "1 0\n",
      "0.9388323426246643\n",
      "1 1\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for i, my_slice_1 in enumerate(slices):\n",
    "    for j, my_slice_2 in enumerate(slices):\n",
    "        print(i, j)\n",
    "        print(1 - cosine(my_sum[j], my_sum[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questa sezione andiamo a prendere un Lexicon già [fatto](https://mpqa.cs.pitt.edu/lexicons/subj_lexicon/). Questo possiede due score: noi chiamiamo 0 quello per l'oggettività, e 1 per la soggettività. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Pre-processing Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:44:39.299297Z",
     "start_time": "2020-09-04T09:44:39.280975Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(\"./lexicon/subjectivityLexicon.csv\", 'r') as file:\n",
    "    lexicon = pd.read_csv(file, engine='c', sep=';', header=None)\n",
    "\n",
    "lexicon.columns = [\"Word\", \"Subj_score\"]\n",
    "\n",
    "# Facciamo un po' di pulizia prima di procedere\n",
    "lexicon[\"Word\"] = lexicon[\"Word\"].apply(lambda x: x.replace(\"word1=\", \"\"))\n",
    "lexicon[\"Subj_score\"] = lexicon[\"Subj_score\"].apply(lambda x: \n",
    "                                                    x.replace(\"type=\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:44:39.304224Z",
     "start_time": "2020-09-04T09:44:39.301188Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def encode_subj(string_score = None):\n",
    "    if string_score == \"weaksubj\":\n",
    "        return(0)\n",
    "    elif string_score == \"strongsubj\":\n",
    "        return(1)\n",
    "    else:\n",
    "        print('[ERROR] Some problems occurd.')\n",
    "        return(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:44:39.312489Z",
     "start_time": "2020-09-04T09:44:39.305987Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lexicon[\"Subj_score\"] = lexicon[\"Subj_score\"].apply(encode_subj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:44:39.335733Z",
     "start_time": "2020-09-04T09:44:39.316029Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(\"./lexicon/clean_lexicon.csv\", 'w') as file:\n",
    "    lexicon.to_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## LRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:44:39.341468Z",
     "start_time": "2020-09-04T09:44:39.338492Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nicoli_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:44:42.520175Z",
     "start_time": "2020-09-04T09:44:39.343253Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Caricamento del lexicon\n",
    "with open(\"./lexicon/clean_lexicon.csv\", 'r') as file:\n",
    "    lexicon = pd.read_csv(file, index_col=1)\n",
    "\n",
    "# Caricamento dei modelli già addestrati\n",
    "slices = {filename.split('/')[-1].replace(\".model\", \"\"): \n",
    "          Word2Vec.load(filename)\n",
    "          for filename in glob.glob('./models/*.model')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:44:42.527399Z",
     "start_time": "2020-09-04T09:44:42.521916Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lexicon = lexicon.drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "lexicon = lexicon.to_dict()[\"Subj_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:44:42.531221Z",
     "start_time": "2020-09-04T09:44:42.528963Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "models_test = [slices[\"New York Times\"], slices[\"Breitbart\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:44:42.534716Z",
     "start_time": "2020-09-04T09:44:42.532746Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "corpora_test = [\"./text_New York Times.txt\", \"./text_Breitbart.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:45:46.963794Z",
     "start_time": "2020-09-04T09:44:42.536221Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count text_New York Times\n",
      "min_count text_New York Times\n",
      "zipf text_New York Times\n",
      "count text_Breitbart\n",
      "min_count text_Breitbart\n",
      "zipf text_Breitbart\n",
      "dataframe done\n",
      "common filter done\n"
     ]
    }
   ],
   "source": [
    "lexicon_refined = lexicon_refinement(lex = lexicon, \n",
    "                   models = models_test, \n",
    "                   corpora = corpora_test, zipf_cutoff=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T13:47:34.473565Z",
     "start_time": "2020-09-03T13:47:34.468831Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "print(len(lexicon_refined))\n",
    "print(len(lexicon_refined[lexicon_refined == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Lexicon Enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Su consiglio di Nicoli, facciamo prima l'arricchimento della classe meno frequente (nel nostro caso quella con `1`). \n",
    "\n",
    "Per eseguirla, usiamo una funzione già costruita dal nostro assistente preferito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:45:46.975794Z",
     "start_time": "2020-09-04T09:45:46.970813Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Dobbiamo fare così per come è costruita la funzione di Nicoli\n",
    "lexicon_refined[lexicon_refined == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:45:46.984399Z",
     "start_time": "2020-09-04T09:45:46.977842Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ability     -1.0\n",
       "able        -1.0\n",
       "actually     1.0\n",
       "against     -1.0\n",
       "agreement   -1.0\n",
       "            ... \n",
       "warning     -1.0\n",
       "well        -1.0\n",
       "white       -1.0\n",
       "will         1.0\n",
       "would       -1.0\n",
       "Name: Valence, Length: 166, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon_refined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-04T09:38:58.194Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/75 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balancing:  75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:47<00:00,  1.51it/s]\n",
      "  0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Growing:  62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:39<00:00,  1.56it/s]\n"
     ]
    }
   ],
   "source": [
    "vectorized_lexicon, lexicon_labels = enrich(lex = lexicon_refined, \n",
    "       models = models_test, \n",
    "       n_target = 300, \n",
    "       msteps = 200, \n",
    "       return_words = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-04T09:38:58.833Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(\"./lexicon/enriched_lexicon.csv\", 'w') as file:\n",
    "    pd.DataFrame({\"Vectorized_words\": vectorized_lexicon.tolist(), \n",
    "              \"Labels\": lexicon_labels}).to_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Induction Nicoli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La procedura di di Nicoli è quella di applicare una **regressione logistica** usando il lessico aumentato come *training set*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: per la score induction andiamo a usare una *Support Vector Machine*. Ci domandiamo quanto questo possa andare bene, oppure se sia qualcosa di sbagliato. È un punto da chiarire e chiedere al prof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:53:39.884694Z",
     "start_time": "2020-09-04T09:53:39.865445Z"
    }
   },
   "outputs": [],
   "source": [
    "# Open the pre-prepared lexicon\n",
    "with open(\"./lexicon/enriched_lexicon.csv\", 'r') as file:\n",
    "    enriched_lexicon = pd.read_csv(file, \n",
    "                                   engine='c', \n",
    "                                   index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:53:45.425261Z",
     "start_time": "2020-09-04T09:53:45.152914Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the strings as lists\n",
    "enriched_lexicon[\"Vectorized_words\"] = enriched_lexicon[\"Vectorized_words\"].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:53:45.787902Z",
     "start_time": "2020-09-04T09:53:45.785356Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9366666666666666"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare a list of 100-dimension vectors (the words)\n",
    "X = enriched_lexicon[\"Vectorized_words\"].tolist()\n",
    "# List of labels (1, -1)\n",
    "y = enriched_lexicon[\"Labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tento con una cross validation e c nell'arco 0.1, 10\n",
    "score_mean = []\n",
    "score_stds = []\n",
    "Cs = Cs = np.arange(1, 20, 1)\n",
    "for c in Cs:\n",
    "    clf = svm.SVC(C=c, kernel = \"poly\", gamma=\"auto\")\n",
    "    scores = cross_val_score(clf, X, y, cv=5)\n",
    "    score_mean.append(scores.mean())\n",
    "    score_stds.append(scores.std())\n",
    "\n",
    "plt.errorbar(Cs, score_mean, yerr = score_stds)\n",
    "plt.legend()\n",
    "plt.title(\"Figuring out best C - Cross validated\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Considerazioni**: c'è un leggero overfitting del modello, ma si può comunque affermare che i risultati sono più che soddisfacienti. La SVM ottiene una accuracy praticamente del 100% sui dati ti training, ovvero riesce a dividere perfettamente l'iperspazio 100-dimensionale. A quanto pare, la divisione dello spazio è molto valida anche per i dati di test, sui quali si riscontra solamente un piccolo errore (si arriva ad ottenere una accuracu sopra il 95%).\n",
    "\n",
    "È importante notare come, possedendo questi addestramenti elementi stocastici, si dovrebbe procedere eseguendo *k-fold cross validation*.\n",
    "\n",
    "Dal grafico si può notare come un valore $C > 5$ sia più che soddisfaciente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA IMPORTANTE**: per ottenere degli intervalli di confidenza, possiamo usare il Bootstrapping proposto da Hamilton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:54:09.665821Z",
     "start_time": "2020-09-04T09:54:09.590162Z"
    }
   },
   "outputs": [],
   "source": [
    "# Addestriamo completamente il lessico\n",
    "inducer = svm.SVC(C = 10, kernel = 'poly', gamma='auto').fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:54:14.302031Z",
     "start_time": "2020-09-04T09:54:10.795753Z"
    }
   },
   "outputs": [],
   "source": [
    "slices = {filename.split('/')[-1].replace(\".model\", \"\"): \n",
    "          Word2Vec.load(filename)\n",
    "          for filename in glob.glob('./models/*.model')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:55:13.746221Z",
     "start_time": "2020-09-04T09:55:13.743740Z"
    }
   },
   "outputs": [],
   "source": [
    "models_test = [slices[\"New York Times\"], slices[\"Breitbart\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:54:16.157888Z",
     "start_time": "2020-09-04T09:54:16.155620Z"
    }
   },
   "outputs": [],
   "source": [
    "corpora_test = [\"./text_New York Times.txt\", \"./text_Breitbart.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:54:31.467800Z",
     "start_time": "2020-09-04T09:54:18.588100Z"
    }
   },
   "outputs": [],
   "source": [
    "# We use the trained indecer to predict a label\n",
    "# for every word in the vocabolary, for all embeddings\n",
    "models_prepagation = [{word: inducer.predict([model.wv[word]])[0] \n",
    "  for word in model.wv.vocab} for model in models_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qua si vanno a provare diverse tecniche di scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tecnica 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-05T10:10:01.502585Z",
     "start_time": "2020-09-05T10:10:01.495190Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e004aa529fe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m vocab_counts = [{word: model.wv.vocab[word].count * model_prop[word]\n\u001b[1;32m      8\u001b[0m                 for word in model.wv.vocab} \n\u001b[0;32m----> 9\u001b[0;31m                 for model, model_prop in zip(models_test, models_prepagation)]\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# This is just the sum of all words in the corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m vocab_counts_abs = [{word: abs(model.wv.vocab[word].count)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'models_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Everything below is done for all embeddings, and \n",
    "# put in a list\n",
    "\n",
    "# This is the sum of the count for every word in the \n",
    "# vocabulary times its label. The total sum should be \n",
    "# an indication for where the corpus lies (subj or obj)\n",
    "vocab_counts = [{word: model.wv.vocab[word].count * model_prop[word]\n",
    "                for word in model.wv.vocab} \n",
    "                for model, model_prop in zip(models_test, models_prepagation)]\n",
    "# This is just the sum of all words in the corpus\n",
    "vocab_counts_abs = [{word: abs(model.wv.vocab[word].count)\n",
    "                    for word in model.wv.vocab}  \n",
    "                    for model, model_prop in zip(models_test, models_prepagation)]\n",
    "# The score is given as the counts weighted over the total\n",
    "# number of words\n",
    "models_score_1 = [sum(count.values())/(sum(count_abs.values()))\n",
    "                for count, count_abs in zip(vocab_counts, vocab_counts_abs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models_score_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tecnica 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qua viene eseguito simile a prime, con la differenza che\n",
    "# però non si contano le parole oggettive\n",
    "def clean_prediction(prediction = None):\n",
    "    if prediction == 1:\n",
    "        return(1)\n",
    "    elif prediction == -1:\n",
    "        return(0)\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "\n",
    "models_prepagation_2 = [{word: \n",
    "                       clean_prediction(inducer.predict([model.wv[word]])[0])\n",
    "  for word in model.wv.vocab} for model in models_test]\n",
    "\n",
    "vocab_counts_2 = [{word: model.wv.vocab[word].count * model_prop[word]\n",
    "                for word in model.wv.vocab} \n",
    "                for model, model_prop in zip(models_test, models_prepagation_2)]\n",
    "\n",
    "\n",
    "# Difatti questa misure dice quante parole soggettive ci sono\n",
    "# per singola parola scritta nei vari corpus.\n",
    "models_score_2 = [sum(count.values())/(sum(count_abs.values()))\n",
    "                for count, count_abs in zip(vocab_counts_2, vocab_counts_abs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models_score_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
