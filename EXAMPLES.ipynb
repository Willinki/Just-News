{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T15:14:59.042261Z",
     "start_time": "2020-09-10T15:14:59.035535Z"
    }
   },
   "outputs": [],
   "source": [
    "# General purpose\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import itertools\n",
    "from numpy.linalg import norm\n",
    "from collections import Counter\n",
    "import codecs\n",
    "import tqdm\n",
    "import scipy as sp\n",
    "from joblib import dump, load\n",
    "import random\n",
    "# NLP\n",
    "from cade.cade import CADE\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "# Machine Learning\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# currently installed theme will be used to\n",
    "# set plot style if no arguments provided\n",
    "# Theme\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Facciamo vedere alcuni esempi di come l'embedding eseguito con CADE, sulle sei diverse testate giornalistiche, dia dei risultati sensati. \n",
    "\n",
    "In particolari, si mostrano casi in cui delle stesse parole cambiano la loro rappresentazione (significato) attraverso i 6 diversi modelli; in questi casi, si mostrano dei casi che possono essere in linea con quanto ci si potrebbe aspettare.\n",
    "È utile mostrare anche dei casi in cui invece si mantiene una parola attraverso i 6 diversi embedding; magari, in tale caso, possiamo usare una parola che appartiene al lessico, e che quindi ci semplifica il lavoro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Si ricordi che gli embedding non dipendono dal *lessico* scelto, e di conseguenza sono agnostici ad essi. L'unica cosa che li determina sono i corpora, che, a meno di errori nella pulizia, e.g. avremmo potuto eseguire della *lemmitization* o cose simili."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Questi confronti presuppongono che Wikipedia sia la fonte più \"da dizionario\" e oggettive tra quelle proposte. Di conseguenza, si usa la sua rappresentazione per mettere a confronto quelle degli altri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T13:21:55.795488Z",
     "start_time": "2020-09-09T13:21:52.592071Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "slices = {filename.split('/')[-1].replace(\".model\", \"\"): \n",
    "          Word2Vec.load(filename)\n",
    "          for filename in glob.glob('./models/*.model')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T13:37:38.819223Z",
     "start_time": "2020-09-09T13:37:38.796748Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia\n",
      "('politics', 1.0)\n",
      "ABC News\n",
      "('politics', 0.558868408203125)\n",
      "Breitbart\n",
      "('philosophy', 0.5695109367370605)\n",
      "CNN\n",
      "('politics', 0.6155990958213806)\n",
      "The Federalist\n",
      "('politics', 0.5537101030349731)\n",
      "New York Times\n",
      "('politics', 0.6331716179847717)\n",
      "News Max\n",
      "('politics', 0.5335795879364014)\n"
     ]
    }
   ],
   "source": [
    "for sl in slices:\n",
    "    print(sl)\n",
    "    print(slices[\"Wikipedia\"].wv.most_similar([slices[sl].wv[\"politics\"]])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Come si può notare, la rappresentazione della parola *politics* in quasi tutti i giornali coincide con la rappresentazione all'interno di wikipedia; l'unico che è differente è Breitbart, nel qual il concetto corrisponde più al *philosophy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T13:45:11.486920Z",
     "start_time": "2020-09-09T13:45:11.464520Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia\n",
      "[('progressive', 0.9999999403953552), ('radical', 0.7494416832923889)]\n",
      "ABC News\n",
      "[('progressive', 0.6467471718788147), ('liberal', 0.6081336736679077)]\n",
      "Breitbart\n",
      "[('progressive', 0.6694939732551575), ('radical', 0.6292732954025269)]\n",
      "CNN\n",
      "[('progressive', 0.688573956489563), ('liberal', 0.6536697149276733)]\n",
      "The Federalist\n",
      "[('progressive', 0.5990332961082458), ('radical', 0.5633199214935303)]\n",
      "New York Times\n",
      "[('progressive', 0.595812201499939), ('liberal', 0.5684530735015869)]\n",
      "News Max\n",
      "[('progressive', 0.6231521368026733), ('liberal', 0.571533203125)]\n"
     ]
    }
   ],
   "source": [
    "for sl in slices:\n",
    "    print(sl)\n",
    "    print(slices[\"Wikipedia\"].\n",
    "          wv.most_similar([slices[sl].wv[\"progressive\"]])[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Questo dà risultati molto interessanti. Come si può notare, la rappresentazione della parola *progressive* è condivisa attraverso tutto gli embedding; quello che però cambia è la seconda parola più simile: se si prende la rappresentazione di un giornale \"di sinistra\", *progressive* tende a essere più vicino al concetto di *liberal* (in Wikipedia); mentre per giornali di destra estrema è più simile a *radical*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T13:49:02.072740Z",
     "start_time": "2020-09-09T13:49:02.050251Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia\n",
      "[('illegal', 1.0), ('unlawful', 0.72054123878479)]\n",
      "ABC News\n",
      "[('illegal', 0.7067723870277405), ('unlawful', 0.5750272274017334)]\n",
      "Breitbart\n",
      "[('illegal', 0.5317680239677429), ('undocumented', 0.47897130250930786)]\n",
      "CNN\n",
      "[('illegal', 0.6380700469017029), ('unlawful', 0.5811516046524048)]\n",
      "The Federalist\n",
      "[('illegal', 0.6033549308776855), ('undocumented', 0.46863484382629395)]\n",
      "New York Times\n",
      "[('illegal', 0.7472089529037476), ('unlawful', 0.6054619550704956)]\n",
      "News Max\n",
      "[('illegal', 0.6044532656669617), ('unlawful', 0.5079938173294067)]\n"
     ]
    }
   ],
   "source": [
    "for sl in slices:\n",
    "    print(sl)\n",
    "    print(slices[\"Wikipedia\"].\n",
    "          wv.most_similar([slices[sl].wv[\"illegal\"]])[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Anche in questo caso, si nota un pattern simile ai precedenti: i giornali di destra tendono ad avere il significato di *illegal* più vicino ad *undocumented* (in Wikipedia), rispetto agli altri che sono su *unlawful*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T14:03:03.617755Z",
     "start_time": "2020-09-09T14:03:03.591944Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia\n",
      "[('god', 1.0), ('gods', 0.8261622190475464)]\n",
      "ABC News\n",
      "[('god', 0.5821242332458496), ('lover', 0.5465329885482788)]\n",
      "Breitbart\n",
      "[('god', 0.6881555914878845), ('jesus', 0.6249061822891235)]\n",
      "CNN\n",
      "[('oh', 0.6258580684661865), ('god', 0.5774052143096924)]\n",
      "The Federalist\n",
      "[('god', 0.5874472260475159), ('thy', 0.515599250793457)]\n",
      "New York Times\n",
      "[('god', 0.6862984895706177), ('heaven', 0.5891343355178833)]\n",
      "News Max\n",
      "[('god', 0.6187192797660828), ('heaven', 0.5652803182601929)]\n"
     ]
    }
   ],
   "source": [
    "for sl in slices:\n",
    "    print(sl)\n",
    "    print(slices[\"Wikipedia\"].\n",
    "          wv.most_similar([slices[sl].wv[\"god\"]])[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Questa piccolo esempio è interessante: la parola ha quasi la stessa rappresentazione per tutti, ma sono leggermente spostasti: la rappresentazione, per esempio, si CNN è più vicina a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T14:04:52.118560Z",
     "start_time": "2020-09-09T14:04:52.094145Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia\n",
      "[('good', 1.0), ('bad', 0.7421819567680359), ('fun', 0.6268191337585449)]\n",
      "ABC News\n",
      "[('good', 0.6347428560256958), ('bad', 0.46216851472854614), ('keen', 0.4531018137931824)]\n",
      "Breitbart\n",
      "[('good', 0.6908650398254395), ('fun', 0.5131772756576538), ('remarkable', 0.5018529891967773)]\n",
      "CNN\n",
      "[('good', 0.6638422608375549), ('bad', 0.5046796798706055), ('fun', 0.5020245313644409)]\n",
      "The Federalist\n",
      "[('good', 0.6423277854919434), ('bad', 0.44417208433151245), ('keen', 0.41980552673339844)]\n",
      "New York Times\n",
      "[('good', 0.7027113437652588), ('keen', 0.5303717851638794), ('bad', 0.49873054027557373)]\n",
      "News Max\n",
      "[('good', 0.6140873432159424), ('bad', 0.5044747591018677), ('fun', 0.4753373861312866)]\n"
     ]
    }
   ],
   "source": [
    "for sl in slices:\n",
    "    print(sl)\n",
    "    print(slices[\"Wikipedia\"].\n",
    "          wv.most_similar([slices[sl].wv[\"good\"]])[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Questa parola rimane costante, anche nella seconda più vicina."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propagations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Without Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Qua si mostrano semplicemente degli esempi di parole che sono state selezionate dopo la procedura di LRP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T14:36:49.345092Z",
     "start_time": "2020-09-10T14:36:49.340168Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(\"./lexicon/lexicon_refined.csv\") as file:\n",
    "    lexicon_refined = pd.read_csv(file)\n",
    "lexicon_refined = lexicon_refined.rename(columns={'Unnamed: 0': \"Word\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T15:03:31.438710Z",
     "start_time": "2020-09-10T15:03:31.433099Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Word, Valence]\n",
       "Index: []"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon_refined[lexicon_refined[\"Word\"] == \"bad\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T15:03:52.988650Z",
     "start_time": "2020-09-10T15:03:52.982007Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>good</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Word  Valence\n",
       "18  good     -1.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon_refined[lexicon_refined[\"Word\"] == \"good\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T14:38:33.177667Z",
     "start_time": "2020-09-10T14:38:33.173330Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########\n",
      "5 subjective words in the lexicon refined.\n",
      "['little', 'will', 'court', 'long', 'like']\n"
     ]
    }
   ],
   "source": [
    "print(\"###########\")\n",
    "print(\"5 subjective words in the lexicon refined.\")\n",
    "print(lexicon_refined[lexicon_refined[\"Valence\"] == 1]\n",
    "      .sample(5)[\"Word\"]\n",
    "      .to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T14:38:33.405235Z",
     "start_time": "2020-09-10T14:38:33.400984Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########\n",
      "5 objective words in the lexicon refined.\n",
      "['least', 'point', 'back', 'so', 'support']\n"
     ]
    }
   ],
   "source": [
    "print(\"###########\")\n",
    "print(\"5 objective words in the lexicon refined.\")\n",
    "print(lexicon_refined[lexicon_refined[\"Valence\"] == -1]\n",
    "      .sample(5)[\"Word\"]\n",
    "      .to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hamilton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T15:05:18.156262Z",
     "start_time": "2020-09-10T15:05:17.815365Z"
    }
   },
   "outputs": [],
   "source": [
    "# We have to remove some nan, because Hamilton sucks\n",
    "propagations = {filename.split(\"_\")[-1].replace(\".csv\", \"\"):\n",
    "                (pd.read_csv(filename, index_col=1).dropna()\n",
    "                .drop(columns=\"Unnamed: 0\").to_dict()[\"Labels\"])\n",
    "                for filename in glob.glob('./propagations/*.csv')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T15:15:58.150209Z",
     "start_time": "2020-09-10T15:15:58.128584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Wikipedia ###\n",
      "['cartoon', 'crowther', 'javanicum', 'uproar', 'tate']\n",
      "### Breitbart ###\n",
      "['railed', 'olly', 'though', 'commenting', 'nhl']\n",
      "### New York Times ###\n",
      "['uptodate', 'todaypresident', 'rr', 'wolcott', 'hoosier']\n",
      "### News Max ###\n",
      "['begged', 'revelation', 'georgetowns', 'case', 'identity']\n",
      "### CNN ###\n",
      "['cafeteria', 'extensions', 'describes', 'fingernails', 'permeated']\n",
      "### The Federalist ###\n",
      "['felon', 'orchestrated', 'twoweek', 'ideals', 'compost']\n",
      "### ABC News ###\n",
      "['overriding', '230c', 'markedly', 'defy', 'midterm']\n"
     ]
    }
   ],
   "source": [
    "# Finding some words\n",
    "for name, prop in propagations.items():\n",
    "    print(\"###\", name, \"###\")\n",
    "    print(random.sample([name for name, val in prop.items() if val > 0.9], \n",
    "                       5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T15:16:34.764684Z",
     "start_time": "2020-09-10T15:16:34.762172Z"
    }
   },
   "outputs": [],
   "source": [
    "test_words = [\"tyrants\", \"decisively\", \n",
    "              \"redeem\", \"unacceptable\", \"snow\"]\n",
    "CORRECTLY_PROPAGATED = \"snow\"\n",
    "WRONGLY_PROPAGATED = \"snow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T15:05:47.424444Z",
     "start_time": "2020-09-10T15:05:47.420432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Wikipedia ###\n",
      "bad 0.26\n",
      "### Breitbart ###\n",
      "bad 0.66\n",
      "### New York Times ###\n",
      "bad 0.12\n",
      "### News Max ###\n",
      "bad 0.06\n",
      "### CNN ###\n",
      "bad 0.93\n",
      "### The Federalist ###\n",
      "bad 0.13\n",
      "### ABC News ###\n",
      "bad 0.31\n"
     ]
    }
   ],
   "source": [
    "for name, prop in propagations.items():\n",
    "    print(\"###\", name, \"###\")\n",
    "    print(CORRECTLY_PROPAGATED, \"{:0.2f}\".format(prop[CORRECTLY_PROPAGATED]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nicoli Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T14:58:55.470499Z",
     "start_time": "2020-09-10T14:58:52.341241Z"
    }
   },
   "outputs": [],
   "source": [
    "slices = {filename.split('/')[-1].replace(\".model\", \"\"): \n",
    "          Word2Vec.load(filename)\n",
    "          for filename in glob.glob('./models/*.model')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T15:08:10.603028Z",
     "start_time": "2020-09-10T15:07:44.279706Z"
    }
   },
   "outputs": [],
   "source": [
    "inducer = load('./ML_models/Nicoli_logistic.joblib')\n",
    "models_propagation_nicoli = {name: {word: \n",
    "                       (inducer.predict_proba([model.wv[word]])[0][1])\n",
    "  for word in model.wv.vocab} for name, model in slices.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T15:17:18.126810Z",
     "start_time": "2020-09-10T15:17:18.124541Z"
    }
   },
   "outputs": [],
   "source": [
    "CORRECTLY_PROPAGATED = \"good\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T15:17:18.281547Z",
     "start_time": "2020-09-10T15:17:18.277212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^^^^ NICOLI ^^^^\n",
      "### Wikipedia ###\n",
      "good 0.1151\n",
      "### ABC News ###\n",
      "good 0.0019\n",
      "### Breitbart ###\n",
      "good 0.7160\n",
      "### CNN ###\n",
      "good 0.7916\n",
      "### The Federalist ###\n",
      "good 0.0000\n",
      "### New York Times ###\n",
      "good 0.0014\n",
      "### News Max ###\n",
      "good 0.3850\n"
     ]
    }
   ],
   "source": [
    "print(\"^^^^ NICOLI ^^^^\")\n",
    "for name, dic in models_propagation_nicoli.items():\n",
    "    print(\"###\", name, '###')\n",
    "    print(CORRECTLY_PROPAGATED, \"{:0.4f}\".format(dic[CORRECTLY_PROPAGATED]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T15:17:18.427365Z",
     "start_time": "2020-09-10T15:17:18.423143Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^^^^ HAMILTON ^^^^\n",
      "### Wikipedia ###\n",
      "good 0.00\n",
      "### Breitbart ###\n",
      "good 0.00\n",
      "### New York Times ###\n",
      "good 0.01\n",
      "### News Max ###\n",
      "good 0.00\n",
      "### CNN ###\n",
      "good 0.05\n",
      "### The Federalist ###\n",
      "good 0.00\n",
      "### ABC News ###\n",
      "good 0.01\n"
     ]
    }
   ],
   "source": [
    "print(\"^^^^ HAMILTON ^^^^\")\n",
    "for name, prop in propagations.items():\n",
    "    print(\"###\", name, \"###\")\n",
    "    print(CORRECTLY_PROPAGATED, \"{:0.2f}\".format(prop[CORRECTLY_PROPAGATED]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
